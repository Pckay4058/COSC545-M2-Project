{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy, pandas as pd, gensim as gn\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore, LdaModel\n",
    "# from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# spacy internal setup\n",
    "if not 'nlp' in locals():\n",
    "    model_loaded: bool = False\n",
    "    while not model_loaded:\n",
    "        # we keep trying until this works...\n",
    "        try:\n",
    "            print(\"attempting to load model\")\n",
    "            # attempt to load the spacy model\n",
    "            nlp: spacy.Language = spacy.load(\"en_core_web_sm\")\n",
    "            print(\"model loaded successfully\")\n",
    "            model_loaded = True\n",
    "        except BaseException as e:\n",
    "            print(\"model failed to load\")\n",
    "            # if we fail to load the model, we are going to make\n",
    "            # sure the package is installed...\n",
    "            from spacy.cli import download\n",
    "            download(\"en_core_web_sm\")\n",
    "            # then loop around and try to load it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root: Path = Path(\".\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_text_raw: str = path_root.joinpath(\"moby_dick.txt\").read_text('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SOME USEFUL VARIABLES FOR PREPROCESSING\n",
    "# utf-8 codes for some characters in the text\n",
    "dq_op: str = \"\\u201C\" # double quotes open\n",
    "dq_cl: str = \"\\u201D\" # double quotes close\n",
    "sq_op: str = \"\\u2018\" # single quote open\n",
    "sq_cl: str = \"\\u2019\" # single quote close\n",
    "under: str = \"\\u005F\" # underscore\n",
    "hyphe: str = \"\\u002D\" # hyphen/minus\n",
    "mdash: str = \"\\u2014\" # em dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace newlines and carriage returns with whitespace\n",
    "md_text: str = re.sub(\"[\\n\\r]\", \" \", md_text_raw)\n",
    "# remove everything before the first chapter\n",
    "md_text = re.sub(\"[\\w\\W]*(?=CHAPTER 1\\. Loomings)\", \"\", md_text, count=1)\n",
    "# remove everything after the end of the epilogue\n",
    "md_text = re.sub(\"\\s*(\\*\\*\\* END)[\\w\\W]+\", \"\", md_text, count=1)\n",
    "# there are some weird embedded books in the middle of the text\n",
    "# first we remove the in-betweens of the embedded books\n",
    "md_text = re.sub(\"(Thus ends BOOK[\\w\\W]+?)([\\w\\W]+?BOOK[\\w\\W]+?)(?=\\s\\s)\", \"\", md_text)\n",
    "# then we get rid of the chapter headings of the embedded books\n",
    "md_text = re.sub(\"BOOK\\s+[IV]+([\\w\\W]+?(?=CHAPTER)CHAPTER\\s+[IV\\d]+\\.\\s+[\\w\\W]+?(?=\\.)\\.)?\", \"\", md_text)\n",
    "# CLEANING UP IMPORTANT UTF-8 CHARACTERS\n",
    "# replace all utf single quotes with ascii single quotes\n",
    "md_text = re.sub(f\"{sq_op}|{sq_cl}\", \"\\'\", md_text)\n",
    "# replace all utf double quotes with ascii double quotes\n",
    "md_text = re.sub(f\"{dq_op}|{dq_cl}\", \"\\\"\", md_text)\n",
    "# replace utf underscores with ascii\n",
    "md_text = re.sub(f\"{under}\", \"_\", md_text)\n",
    "# replace utf hyphens with ascii\n",
    "md_text = re.sub(f\"{hyphe}\", \"-\", md_text)\n",
    "# replace utf em dash with hyphen\n",
    "md_text = re.sub(f\"{mdash}\", \"-\", md_text)\n",
    "# next (and I don't know if this is a good idea...) we're going to replace\n",
    "# all punctuation that is NOT a:\n",
    "# . or ! or ? or whitespace or \"\n",
    "md_text = re.sub(\"[^\\w\\d\\s\\.\\\"?!]\", \" \", md_text)\n",
    "# finally, we want to replace repeated whitespace with single whitespace\n",
    "md_text = re.sub(\"\\s+\", \" \", md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to contain intermediate variables\n",
    "def split_chapters(text: str) -> dict[str, str]:\n",
    "    # build a list of the chapter titles and their contents\n",
    "    list_results: list[str] = [result.strip() for result in re.split(\"(CHAPTER\\s\\d+\\.\\s)|(Epilogue)\", text) if result != \"\" and result is not None]\n",
    "    # use that list to create a dictionary that is {chapter_title: chapter_content}\n",
    "    return {x: list_results[i+1] for i, x in enumerate(list_results) if re.match(\"(CHAPTER\\s\\d+\\.)|(Epilogue)\", x)}\n",
    "# run the processed text through the chapter splitter\n",
    "dict_chapters: dict[str, str] = split_chapters(md_text)\n",
    "# dict_chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the spacy nlp object\n",
    "# this object contains all of the functionality for\n",
    "# turning a string or list of strings into spacy \"Documents\"\n",
    "\"\"\"we no longer need to do the following line as it's loaded with the imports\"\"\"\n",
    "# nlp: spacy.Language = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# we CAN add pipelines here though\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "# our book is a little too long for the default\n",
    "# processing limit so we increase it slightly\n",
    "nlp.max_length = 1200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let spacy process the full text\n",
    "# this produces a \"Document\"\n",
    "# the document is a tokenized representation of the full\n",
    "# text with a bunch of extra information attached to the tokens\n",
    "doc = nlp(md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy stores things like parts of speech as integer tags\n",
    "# we can use this reverse lookup table to get the string label\n",
    "lookup: dict[int, str] = {y: x for x, y in spacy.symbols.IDS.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now build a list of all things that are like nouns for the full text\n",
    "noun_alikes: list[str] = [x.text for x in doc if lookup[x.pos] in {\"NOUN\", \"PROPN\", \"PRON\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas we build a series from the noun_alikes list\n",
    "nouns: pd.Series = pd.Series(noun_alikes)\n",
    "# that we can use to get the unique nouns and how many times they appear in the text\n",
    "nouns_counts: pd.Series = nouns.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "it                2187\n",
       "I                 2074\n",
       "he                1643\n",
       "him               1054\n",
       "that               863\n",
       "                  ... \n",
       "one voyage           1\n",
       "the logs             1\n",
       "many hunters         1\n",
       "given waters         1\n",
       "another orphan       1\n",
       "Name: count, Length: 27214, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_mod = gensim.models.phrases.Phraser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CHAPTER, 1, .]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in list(doc.sents)[0].as_doc()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# culling the tokens to get rid of the punctuation tokens and stopwords.\n",
    "tokens_filtered: list[str] = [\n",
    "    token.lemma_ for token in doc if \n",
    "    not token.is_stop\n",
    "    and not token.is_punct\n",
    "    and not token.is_digit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts: list[list[str]] = [\n",
    "    [\n",
    "        token.text for token in sentence.as_doc() if\n",
    "        not token.is_stop\n",
    "        and not token.is_punct\n",
    "        and not token.is_digit\n",
    "    ]\n",
    "    for sentence in doc.sents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=20, \n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.046*\"aloft\" + 0.032*\"high\" + 0.013*\"right\" + 0.012*\"leaning\" + 0.011*\"looking\" + 0.007*\"the bulwarks\" + 0.002*\"seated\" + 0.002*\"ships\" + 0.000*\"Ahab\" + 0.000*\"s\"'),\n",
       " (1,\n",
       "  '0.044*\"way\" + 0.023*\"fixed\" + 0.022*\"all this\" + 0.011*\"view\" + 0.010*\"try\" + 0.004*\"needs\" + 0.003*\"my soul\" + 0.002*\"bringing\" + 0.002*\"knowing\" + 0.002*\"served\"'),\n",
       " (2,\n",
       "  '0.219*\"like\" + 0.024*\"goes\" + 0.007*\"order\" + 0.006*\"jump\" + 0.004*\"sleep\" + 0.000*\"sleeps\" + 0.000*\"s\" + 0.000*\"Ahab\" + 0.000*\"Stubb\" + 0.000*\"upwards\"'),\n",
       " (3,\n",
       "  '0.044*\"t\" + 0.039*\"long\" + 0.026*\"nigh\" + 0.024*\"thought\" + 0.021*\"sail\" + 0.017*\"years\" + 0.017*\"ago\" + 0.009*\"taking\" + 0.008*\"mind\" + 0.005*\"bear\"'),\n",
       " (4,\n",
       "  '0.029*\"grow\" + 0.021*\"little\" + 0.015*\"pay\" + 0.008*\"the same time\" + 0.008*\"mean\" + 0.000*\"s\" + 0.000*\"smaller\" + 0.000*\"sir\" + 0.000*\"eight and forty hours\" + 0.000*\"the whale\"'),\n",
       " (5,\n",
       "  '0.038*\"tell\" + 0.000*\"Ahab\" + 0.000*\"thee\" + 0.000*\"s\" + 0.000*\"muttered\" + 0.000*\"the whale\" + 0.000*\"swim\" + 0.000*\"these sharks\" + 0.000*\"feast\" + 0.000*\"day\"'),\n",
       " (6,\n",
       "  '0.060*\"come\" + 0.000*\"s\" + 0.000*\"thee\" + 0.000*\"let\" + 0.000*\"cried\" + 0.000*\"Oh\" + 0.000*\"the sharks\" + 0.000*\"O\" + 0.000*\"my master\" + 0.000*\"the low cabin window\"'),\n",
       " (7,\n",
       "  '0.052*\"came\" + 0.030*\"hold\" + 0.007*\"vain\" + 0.006*\"exactly\" + 0.004*\"a sort\" + 0.000*\"Ahab\" + 0.000*\"lances\" + 0.000*\"s\" + 0.000*\"the boats\" + 0.000*\"his tail\"'),\n",
       " (8,\n",
       "  '0.080*\"the ship\" + 0.041*\"men\" + 0.025*\"water\" + 0.020*\"Let\" + 0.017*\"set\" + 0.009*\"that man\" + 0.004*\"infallibly\" + 0.003*\"his legs\" + 0.002*\"quietly\" + 0.001*\"his feet\"'),\n",
       " (9,\n",
       "  '0.067*\"heard\" + 0.054*\"soon\" + 0.019*\"sea\" + 0.012*\"find\" + 0.004*\"account\" + 0.003*\"especially\" + 0.003*\"pausing\" + 0.000*\"ye\" + 0.000*\"Ahab\" + 0.000*\"s\"'),\n",
       " (10,\n",
       "  '0.067*\"gone\" + 0.022*\"left\" + 0.000*\"Ahab\" + 0.000*\"the Parsee\" + 0.000*\"The Parsee\" + 0.000*\"the whale\" + 0.000*\"the boat\" + 0.000*\"clear\" + 0.000*\"Starbuck\" + 0.000*\"Tashtego\"'),\n",
       " (11,\n",
       "  '0.048*\"stand\" + 0.034*\"the water\" + 0.027*\"straight\" + 0.025*\"speak\" + 0.014*\"making\" + 0.011*\"the world\" + 0.009*\"hours\" + 0.007*\"previous\" + 0.004*\"bound\" + 0.003*\"putting\"'),\n",
       " (12,\n",
       "  '0.000*\"the Highland hunters\" + 0.000*\"the antlered thoughts\" + 0.000*\"the brow\" + 0.000*\"the snow line\" + 0.000*\"Melancthon\" + 0.000*\"Shakespeare s\" + 0.000*\"clear eternal tideless mountain lakes\" + 0.000*\"alpine land\" + 0.000*\"the foreheads\" + 0.000*\"the snow prints\"'),\n",
       " (13,\n",
       "  '0.033*\"Look\" + 0.009*\"leaves\" + 0.003*\"carries\" + 0.000*\"stars\" + 0.000*\"ye sun moon\" + 0.000*\"Ahab\" + 0.000*\"ye\" + 0.000*\"thou underling\" + 0.000*\"no dust\" + 0.000*\"the gait\"'),\n",
       " (14,\n",
       "  '0.060*\"far\" + 0.042*\"sight\" + 0.039*\"feel\" + 0.016*\"told\" + 0.008*\"the quarter deck\" + 0.003*\"land\" + 0.002*\"the sailors\" + 0.001*\"the most part\" + 0.000*\"the whale\" + 0.000*\"Ahab\"'),\n",
       " (15,\n",
       "  '0.129*\"round\" + 0.000*\"Ahab\" + 0.000*\"the whale\" + 0.000*\"turned\" + 0.000*\"s\" + 0.000*\"the boat\" + 0.000*\"close\" + 0.000*\"the Parsee\" + 0.000*\"seen\" + 0.000*\"the lines\"'),\n",
       " (16,\n",
       "  '0.031*\"think\" + 0.027*\"suddenly\" + 0.026*\"passed\" + 0.025*\"run\" + 0.021*\"knew\" + 0.017*\"deep\" + 0.011*\"all hands\" + 0.003*\"sadly\" + 0.002*\"receiving\" + 0.002*\"invest\"'),\n",
       " (17,\n",
       "  '0.000*\"the Highland hunters\" + 0.000*\"the antlered thoughts\" + 0.000*\"the brow\" + 0.000*\"the snow line\" + 0.000*\"Melancthon\" + 0.000*\"Shakespeare s\" + 0.000*\"clear eternal tideless mountain lakes\" + 0.000*\"alpine land\" + 0.000*\"the foreheads\" + 0.000*\"the snow prints\"'),\n",
       " (18,\n",
       "  '0.046*\"time\" + 0.026*\"going\" + 0.016*\"drawn\" + 0.015*\"Yes\" + 0.007*\"knows\" + 0.004*\"orders\" + 0.004*\"formed\" + 0.003*\"every one\" + 0.001*\"the decks\" + 0.000*\"s\"'),\n",
       " (19,\n",
       "  '0.122*\"the sea\" + 0.062*\"saw\" + 0.027*\"heaven\" + 0.020*\"better\" + 0.012*\"having\" + 0.006*\"believe\" + 0.006*\"a man\" + 0.002*\"considering\" + 0.002*\"answer\" + 0.000*\"earnestly\"')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis as vis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "vis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pyLDAvis._prepare.prepare() argument after ** must be a mapping, not LdaModel",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vis\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlda_model)\n",
      "\u001b[1;31mTypeError\u001b[0m: pyLDAvis._prepare.prepare() argument after ** must be a mapping, not LdaModel"
     ]
    }
   ],
   "source": [
    "vis.gensimvis.prepare(lda_model, corpus, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
