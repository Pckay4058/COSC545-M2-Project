{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to load model\n",
      "model failed to load\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "attempting to load model\n",
      "model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy, pandas as pd\n",
    "\n",
    "# we're downloading a pre-trained spacy model and loading it\n",
    "model_loaded: bool = False\n",
    "while not model_loaded:\n",
    "    try:\n",
    "        print(\"attempting to load model\")\n",
    "        nlp: spacy.Language = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"model loaded successfully\")\n",
    "        model_loaded = True\n",
    "    except BaseException as e:\n",
    "        print(\"model failed to load\")\n",
    "        from spacy.cli import download\n",
    "        download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root: Path = Path(\".\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_text_raw: str = path_root.joinpath(\"moby_dick.txt\").read_text('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SOME USEFUL VARIABLES FOR PREPROCESSING\n",
    "# utf-8 codes for some characters in the text\n",
    "dq_op: str = \"\\u201C\" # double quotes open\n",
    "dq_cl: str = \"\\u201D\" # double quotes close\n",
    "sq_op: str = \"\\u2018\" # single quote open\n",
    "sq_cl: str = \"\\u2019\" # single quote close\n",
    "under: str = \"\\u005F\" # underscore\n",
    "hyphe: str = \"\\u002D\" # hyphen/minus\n",
    "mdash: str = \"\\u2014\" # em dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace newlines and carriage returns with whitespace\n",
    "md_text: str = re.sub(\"[\\n\\r]\", \" \", md_text_raw)\n",
    "# remove everything before the first chapter\n",
    "md_text = re.sub(\"[\\w\\W]*(?=CHAPTER 1\\. Loomings)\", \"\", md_text, count=1)\n",
    "# remove everything after the end of the epilogue\n",
    "md_text = re.sub(\"\\s*(\\*\\*\\* END)[\\w\\W]+\", \"\", md_text, count=1)\n",
    "# there are some weird embedded books in the middle of the text\n",
    "# first we remove the in-betweens of the embedded books\n",
    "md_text = re.sub(\"(Thus ends BOOK[\\w\\W]+?)([\\w\\W]+?BOOK[\\w\\W]+?)(?=\\s\\s)\", \"\", md_text)\n",
    "# then we get rid of the chapter headings of the embedded books\n",
    "md_text = re.sub(\"BOOK\\s+[IV]+([\\w\\W]+?(?=CHAPTER)CHAPTER\\s+[IV\\d]+\\.\\s+[\\w\\W]+?(?=\\.)\\.)?\", \"\", md_text)\n",
    "# CLEANING UP IMPORTANT UTF-8 CHARACTERS\n",
    "# replace all utf single quotes with ascii single quotes\n",
    "md_text = re.sub(f\"{sq_op}|{sq_cl}\", \"\\'\", md_text)\n",
    "# replace all utf double quotes with ascii double quotes\n",
    "md_text = re.sub(f\"{dq_op}|{dq_cl}\", \"\\\"\", md_text)\n",
    "# replace utf underscores with ascii\n",
    "md_text = re.sub(f\"{under}\", \"_\", md_text)\n",
    "# replace utf hyphens with ascii\n",
    "md_text = re.sub(f\"{hyphe}\", \"-\", md_text)\n",
    "# replace utf em dash with hyphen\n",
    "md_text = re.sub(f\"{mdash}\", \"-\", md_text)\n",
    "# next (and I don't know if this is a good idea...) we're going to replace\n",
    "# all punctuation that is NOT a:\n",
    "# . or ! or ? or whitespace or \"\n",
    "md_text = re.sub(\"[^\\w\\d\\s\\.\\\"?!]\", \" \", md_text)\n",
    "# finally, we want to replace repeated whitespace with single whitespace\n",
    "md_text = re.sub(\"\\s+\", \" \", md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to contain intermediate variables\n",
    "def split_chapters(text: str) -> dict[str, str]:\n",
    "    # build a list of the chapter titles and their contents\n",
    "    list_results: list[str] = [result.strip() for result in re.split(\"(CHAPTER\\s\\d+\\.\\s)|(Epilogue)\", text) if result != \"\" and result is not None]\n",
    "    # use that list to create a dictionary that is {chapter_title: chapter_content}\n",
    "    return {x: list_results[i+1] for i, x in enumerate(list_results) if re.match(\"(CHAPTER\\s\\d+\\.)|(Epilogue)\", x)}\n",
    "# run the processed text through the chapter splitter\n",
    "dict_chapters: dict[str, str] = split_chapters(md_text)\n",
    "# dict_chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the spacy nlp object\n",
    "# this object contains all of the functionality for\n",
    "# turning a string or list of strings into spacy \"Documents\"\n",
    "\"\"\"we no longer need to do the following line as it's loaded with the imports\"\"\"\n",
    "# nlp: spacy.Language = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# our book is a little too long for the default\n",
    "# processing limit so we increase it slightly\n",
    "nlp.max_length = 1200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let spacy process the full text\n",
    "# this produces a \"Document\"\n",
    "# the document is a tokenized representation of the full\n",
    "# text with a bunch of extra information attached to the tokens\n",
    "doc = nlp(md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy stores things like parts of speech as integer tags\n",
    "# we can use this reverse lookup table to get the string label\n",
    "lookup: dict[int, str] = {y: x for x, y in spacy.symbols.IDS.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now build a list of all things that are like nouns for the full text\n",
    "noun_alikes: list[str] = [x.text for x in doc if lookup[x.pos] in {\"NOUN\", \"PROPN\", \"PRON\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas we build a series from the noun_alikes list\n",
    "nouns: pd.Series = pd.Series(noun_alikes)\n",
    "# that we can use to get the unique nouns and how many times they appear in the text\n",
    "nouns_counts: pd.Series = nouns.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "his           2439\n",
       "it            2193\n",
       "I             2080\n",
       "he            1651\n",
       "him           1055\n",
       "              ... \n",
       "damage           1\n",
       "northwest        1\n",
       "nineteenth       1\n",
       "fur              1\n",
       "orphan           1\n",
       "Name: count, Length: 10116, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
